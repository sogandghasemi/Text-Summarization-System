{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bec24f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2ce6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch content from Wikipedia for a given subject\n",
    "def fetch_wikipedia_content(subject):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('english')\n",
    "    page_py = wiki_wiki.page(subject)\n",
    "\n",
    "    if not page_py.exists():\n",
    "        print(f\"Wikipedia page for '{subject}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    return page_py.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "922b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save content to a file\n",
    "def save_to_file(content, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "453560be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read content from a file\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec01b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize a text within the context window\n",
    "def summarize_text(text, target_length):\n",
    "    summary = \"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    length = 0\n",
    "    for sentence in sentences:\n",
    "        if length + len(sentence.split()) <= target_length:\n",
    "            summary += sentence + \" \"\n",
    "            length += len(sentence.split())\n",
    "        else:\n",
    "            break\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(content, output_filename, context_window_limit):\n",
    "    length = len(content.split())\n",
    "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
    "\n",
    "    summary_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < length:\n",
    "        slice_content = content[start_index:]\n",
    "        summary_slice = summarize_text(slice_content, context_window_limit)\n",
    "        summary_list.append(summary_slice)\n",
    "        start_index += len(summary_slice.split())\n",
    "\n",
    "    collated_summary = \" \".join(summary_list)\n",
    "\n",
    "    while len(collated_summary.split()) > context_window_limit:\n",
    "        collated_summary = summarize_text(collated_summary, context_window_limit)\n",
    "\n",
    "    with open(output_filename, \"w\") as file:\n",
    "        file.write(collated_summary)\n",
    "\n",
    "    return collated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4d1332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Take input from the user for the Wikipedia subjects\n",
    "    subject1 = input(\"Enter the first subject: \")\n",
    "    subject2 = input(\"Enter the second subject: \")\n",
    "\n",
    "    # Fetch content from Wikipedia\n",
    "    content1 = fetch_wikipedia_content(subject1)\n",
    "    content2 = fetch_wikipedia_content(subject2)\n",
    "\n",
    "    if content1 and content2:\n",
    "        # Save content to input files\n",
    "        save_to_file(content1, \"input_text1.txt\")\n",
    "        save_to_file(content2, \"input_text2.txt\")\n",
    "\n",
    "        # Summarize text files\n",
    "        contents_1 = read_from_file(\"input_text1.txt\")\n",
    "        contents_2 = read_from_file(\"input_text2.txt\")\n",
    "\n",
    "        perform_summarization(contents_1, \"summarized_text1.txt\", 128)\n",
    "        perform_summarization(contents_2, \"summarized_text2.txt\", 128)\n",
    "\n",
    "        # Generate the query\n",
    "        query = f\"\\nDocument 1 summary: {read_from_file('summarized_text1.txt')}\\n\\nDocument 2 summary: {read_from_file('summarized_text2.txt')}\"\n",
    "\n",
    "        print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "091811ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first subject: Natural Language Processing\n",
      "Enter the second subject: Artificial Intelligence\n",
      "\n",
      "Document 1 summary: Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation. History\n",
      "Natural language processing has its roots in the 1950s.\n",
      "\n",
      "Document 2 summary: Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or other living beings. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs. AI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. living beings.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a25340",
   "metadata": {},
   "source": [
    "Adding cosine similarity for enhancing the quality and relevance of the generated summary by considering the relationships between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52c347d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8763f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(sentences):\n",
    "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8337b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize a text within the context window using cosine similarity\n",
    "def summarize_text_cosine_similarity(text, target_length):\n",
    "    sentences = sent_tokenize(text)\n",
    "    length = 0\n",
    "    summary = \"\"\n",
    "    \n",
    "    # Compute cosine similarity matrix between sentences\n",
    "    similarity_matrix = compute_similarity_matrix(sentences)\n",
    "    \n",
    "    while length < target_length and length < len(sentences):\n",
    "        # Find the sentence most similar to the existing summary\n",
    "        most_similar_index = similarity_matrix[length].argsort()[-1]\n",
    "        \n",
    "        # Add the selected sentence to the summary\n",
    "        summary += sentences[most_similar_index] + \" \"\n",
    "        \n",
    "        # Update the length based on the added sentence\n",
    "        length += len(sentences[most_similar_index].split())\n",
    "\n",
    "        # Set the similarity scores of the added sentence to 0 to avoid repetition\n",
    "        similarity_matrix[:, most_similar_index] = 0\n",
    "\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b450c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(content, output_filename, context_window_limit):\n",
    "    length = len(content.split())\n",
    "    target_length = int(length * (context_window_limit / (context_window_limit + 4000)))\n",
    "\n",
    "    summary = summarize_text_cosine_similarity(content, target_length)\n",
    "\n",
    "    with open(output_filename, \"w\") as file:\n",
    "        file.write(summary)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c32d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 summary: Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. containing words or structures that have not been seen before) and erroneous input (e.g. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). Syntactic analysis\n",
      "Grammar induction\n",
      "Generate a formal grammar that describes a language's syntax. Furthermore, many other languages in non-Western scripts (e.g. from a dictionary or an online resource such as WordNet. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. It has thus been subject to a number of shared tasks since 2011.\n",
      "\n",
      "Document 2 summary: Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or other living beings. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.A knowledge base is a body of knowledge represented in a form that can be used by a program. \"Heuristics\" or \"rules of thumb\" can help to prioritize choices that are more likely to reach a goal.Adversarial search is used for game-playing programs, such as chess or Go. The decision tree is the simplest and most widely used symbolic machine learning algorithm. The multiple layers can progressively extract higher-level features from the raw input. Specialized hardware and software\n",
      "In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software, had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. \"Criticism of COMPAS highlighted a deeper problem with the misuse of AI. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.People who have been harmed by an algorithm's decision have a right to an explanation. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate both mathematical deduction and formal reasoning, which is known as the ChurchTuring thesis. Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\".\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "        contents_1 = read_from_file(\"input_text1.txt\")\n",
    "        contents_2 = read_from_file(\"input_text2.txt\")\n",
    "\n",
    "        perform_summarization(contents_1, \"Second_summarized_text1.txt\", 128)\n",
    "        perform_summarization(contents_2, \"Second_summarized_text2.txt\", 128)\n",
    "\n",
    "        # Generate the query\n",
    "        query = f\"\\nDocument 1 summary: {read_from_file('Second_summarized_text1.txt')}\\n\" \\\n",
    "                f\"\\nDocument 2 summary: {read_from_file('Second_summarized_text2.txt')}\"\n",
    "\n",
    "        print(query)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca776826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
